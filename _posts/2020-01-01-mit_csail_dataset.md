---
access: 'ZIP archives with eye tracking database: Set of stimuli: Link: http://people.csail.mit.edu/tjudd/WherePeopleLook/ALLSTIMULI.zip
  Eye tracking data: http://people.csail.mit.edu/tjudd/WherePeopleLook/DATA.zip Human
  fixation maps: http://people.csail.mit.edu/tjudd/WherePeopleLook/ALLFIXATIONMAPS.zip'
author: MIT
categories:
- Image
citation: ''
contact_email: null
contact_name: Judd Tilke (tjudd@mit.edu)
database: MIT CSAIL Dataset
excerpt: ''
external_link: http://people.csail.mit.edu/tjudd/WherePeopleLook/index.html
hrc: ''
license: ''
method: ''
other: Eyetracking
partner: false
publicly_available: true
ratings: 15.0
references:
  TED09: Tilke, J., Ehinger, K., Durand, F., Torralba, A. Learning to predict where
    humans look, 2009, Proceedings of the IEEE International Conference on Computer
    Vision , art. no. 5459462, pp. 2106-2113.
resolution: ''
src: ''
subjective_scores: false
tags:
- Image
title: MIT CSAIL Dataset
total: 1003
---

For many applications in graphics, design, and human computer interaction, it is essential to understand where humans look in a scene. Where eye tracking devices are not a viable option, models of saliency can be used to predict fixation locations. Most saliency approaches are based on bottom-up computation that does not consider top-down image semantics and often does not match actual eye movements. The database contains collected eye tracking data of 15 viewers on 1003 images and use this database as training and testing examples to learn a model of saliency based on low, middle and high-level image features. This large database of eye tracking data is publicly available with this paper.